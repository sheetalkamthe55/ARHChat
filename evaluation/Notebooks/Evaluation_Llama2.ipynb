{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ae3069-6d29-436f-8d4a-a6733a340547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 20 17:04:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   37C    P8              21W / 450W |   3228MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:61:00.0 Off |                  Off |\n",
      "|  0%   41C    P2             111W / 450W |   2822MiB / 24564MiB |     37%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1582      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1732      G   /usr/bin/gnome-shell                         12MiB |\n",
      "|    0   N/A  N/A    571303      C   /llama-server                              3132MiB |\n",
      "|    1   N/A  N/A      1582      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    571303      C   /llama-server                               428MiB |\n",
      "|    1   N/A  N/A   1670295    C+G   ...aries/Linux/CarlaUE4-Linux-Shipping     2306MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6caf329-bdb4-48c2-a256-241ad2a44fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/sheetal/Project/ARHChat/utils/')\n",
    "from chat_with_history_handler import MessageHistoryHandler\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f10b7-c4fe-4b9d-b9fd-63f72daf975f",
   "metadata": {},
   "source": [
    "BLEU Score: Measures the similarity between a generated sentence and a reference sentence.\n",
    "METEOR Score: Considers synonyms, stemming, and more.\n",
    "ROUGE Score: Measures the overlap between a generated summary and a reference summary.\n",
    "SentenceSim Score: Compares semantic similarity between sentences.\n",
    "SimHash Score: Used for duplicate detection.\n",
    "Perplexity Score: Measures how well the model predicts the sample.\n",
    "BLEURT Score: Evaluates text generation quality.\n",
    "F1 Score: Harmonic mean of precision and recall.\n",
    "BERT Score: Considers contextual embeddings from BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227625d6-169b-419e-a2f2-3ccbaad60fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = MessageHistoryHandler(mongodb_uri=\"mongodb+srv://sheetal:Sheetal%40123@ragcluster.uxamzan.mongodb.net/?retryWrites=true&w=majority&appName=RAGCluster\",\n",
    "    db_name=\"Inference_chatbot\",\n",
    "    vector_db_url=\"https://cd253bb8-d8cf-4818-b78b-981e8b0f40f3.us-east4-0.gcp.cloud.qdrant.io:6333/\",\n",
    "    vector_db_apikey=\"KvgaQh-g2wwl2tUkUyQL2RoRfccqRBSbFsJhosO2rnmO87pH8VfcNA\",\n",
    "    vector_db_collection_name=\"ARH_Llama\",\n",
    "    inference_server_url=\"http://129.69.217.24:8009/v1\",\n",
    "    embedding_model_name=\"intfloat/e5-base-v2\"\n",
    ")\n",
    "with_message_history = handler.with_message_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace2c793-de98-476f-8250-af0547ab1879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BPM stands for Business Process Management. It refers to the management and optimization of business processes, which are sets of activities that create value for customers or stakeholders. BPM involves modeling, analyzing, executing, monitoring, and optimizing these processes to achieve specific goals and objectives.\\n\\nIn a broader sense, BPM encompasses various aspects, including:\\n\\n1. **Process Modeling**: Creating visual representations of business processes using tools like Business Process Model and Notation (BPMN).\\n2. **Process Analysis**: Identifying ineff'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = with_message_history.invoke({\"question\": \"What is BPM?\"},config={\"configurable\": {\"session_id\": \"abc123\",\"user_id\": \"user1\"}})\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa0f3fa-5494-4bd2-9d78-78c67561a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "Microservices Identification, Service Cutter use static analysis techniques, while only IBM’s Mono2Micro and MonoBreaker apply dynamic analysis of the monolith in addition. Three out of five tools are limited to Java-based source code, one requires Python input, while Service Cutter is the only language-agnostic tool.\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('/home/sheetal/Project/lexicaleval.csv')\n",
    "# for i in range(len(df[\"question\"])):\n",
    "\n",
    "#         print(df[\"question\"][i])\n",
    "#         print(df[\"ideal_answer\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fc0b78-3568-432a-ae5d-c9503b7cdde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "MONGODB_URI = \"mongodb+srv://sheetal:Sheetal%40123@ragcluster.uxamzan.mongodb.net/?retryWrites=true&w=majority&appName=RAGCluster\"\n",
    "DB_NAME = \"Inference_chatbot\"\n",
    "mclient = MongoClient(MONGODB_URI)\n",
    "database = mclient[DB_NAME]\n",
    "collection = database[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63037851-96cb-4c8a-87a6-11616cc335b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             question  \\\n",
      "0   How practically usable are existing tools for ...   \n",
      "1   What business-oriented quality goals should we...   \n",
      "2   How can we ensure compatibility between existi...   \n",
      "3   What measures can we take to maintain the qual...   \n",
      "4   What portability considerations are relevant f...   \n",
      "5   How can we enhance reliability and scalability...   \n",
      "6   What security measures should be in place for ...   \n",
      "7   How can we achieve autonomy for each microserv...   \n",
      "8   What cohesion and coupling levels are desirabl...   \n",
      "9   How do we manage complexity and granularity in...   \n",
      "10  What isolation mechanisms can we implement to ...   \n",
      "11  How should we handle technology heterogeneity ...   \n",
      "12  What guidelines or workflows can we establish ...   \n",
      "13  How should we represent the list of services t...   \n",
      "14  Can we extract relevant portions of the monoli...   \n",
      "15  What recommendations exist for splitting the m...   \n",
      "16  How can we visualize the microservices and the...   \n",
      "17  The monolith exposes APIs or interfaces. How c...   \n",
      "18  How to deal with database schema or data stora...   \n",
      "19  How is the monolith’s source code refactored i...   \n",
      "20  What test cases exist for the monolith, and ho...   \n",
      "21  What is the best tool in java to refactor our ...   \n",
      "22                                    What is Docker?   \n",
      "23                            What is an API Gateway?   \n",
      "24        What is Service Discovery in microservices?   \n",
      "25         What is Circuit Breaking in microservices?   \n",
      "26  What do you recommend? Migration of services s...   \n",
      "\n",
      "                                         ideal_answer  \n",
      "0   Microservices Identification, Service Cutter u...  \n",
      "1   Accelerating the delivery of new features and ...  \n",
      "2   Design well-defined APIs for the microservices...  \n",
      "3   Implement CI/CD pipelines to automate testing,...  \n",
      "4   Use container orchestration platforms like Kub...  \n",
      "5   Implement load balancers to distribute incomin...  \n",
      "6   Implement secure authentication mechanisms lik...  \n",
      "7   Define clear boundaries for each microservice ...  \n",
      "8   Aim for high cohesion within each microservice...  \n",
      "9   Microservices Decomposition: Break down comple...  \n",
      "10  Utilize container technologies to isolate micr...  \n",
      "11  Introduce an API gateway to abstract technolog...  \n",
      "12  Establish a clear plan outlining the migration...  \n",
      "13  Maintain a service registry or catalog listing...  \n",
      "14  Identify cohesive modules within the monolith ...  \n",
      "15  Apply Domain-Driven Design(DDD) principles to ...  \n",
      "16  Use tools like Graphviz or architecture visual...  \n",
      "17  Analyze the monolith's APIs to identify distin...  \n",
      "18  Analyze the monolith's database schema to unde...  \n",
      "19  Identify cohesive modules within the monolith ...  \n",
      "20  Review the test suite of the monolith to ident...  \n",
      "21  A tool called MicroRefact to automatically evo...  \n",
      "22  Docker is an open source platform that enables...  \n",
      "23  An API gateway is a data-plane entry point for...  \n",
      "24  A Service Discovery component acts as a regist...  \n",
      "25  The Circuit Breaker pattern in microservices a...  \n",
      "26  According to the paper, the authors recommend ...  \n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('/home/sheetal/Project/lexicaleval.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46ed5da-a638-485c-8f23-d73281a30a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_evaluation_script(data_path, question_column, idea_answer_column, result_file):\n",
    "\n",
    "    print(\"Data Path:\", data_path)\n",
    "    print(\"Question Column:\", question_column)\n",
    "    print(\"Idea Answer Column:\", idea_answer_column)\n",
    "    print(\"Result File:\", result_file)\n",
    "\n",
    "    # Get the file extension\n",
    "    file_extension = os.path.splitext(data_path)[1].lower()\n",
    "\n",
    "    # Load data based on file extension\n",
    "    if file_extension == \".json\":\n",
    "        df = pd.read_json(data_path)\n",
    "    elif file_extension in [\".csv\", \".txt\"]:\n",
    "        df = pd.read_csv(data_path)\n",
    "    elif file_extension in [\".xls\", \".xlsx\"]:\n",
    "        df = pd.read_excel(data_path)\n",
    "    else:\n",
    "        print(\"Error: The file format is not supported.\")\n",
    "\n",
    "    ans = []\n",
    "    qs=[]\n",
    "    ians=[]\n",
    "\n",
    "    count=0\n",
    "    max_retries = 3 \n",
    "    for i in range(len(df[question_column])):\n",
    "\n",
    "        question = df[question_column][i]\n",
    "        ideal_answer = df[idea_answer_column][i]\n",
    "        \n",
    "        collection.delete_many({'SessionId': 'abc1234'})\n",
    "\n",
    "        retries = 0\n",
    "        while retries < max_retries and count <= 1000:\n",
    "            try:\n",
    "\n",
    "                print(\"---------- question\",question)\n",
    "                print(\"---------- question no:\",count)\n",
    "                start_time = time.time()\n",
    "                response = with_message_history.invoke({\"question\": question},config={\"configurable\": {\"session_id\": \"abc1234\",\"user_id\": \"user1\"}})\n",
    "                end_time = time.time()\n",
    "                time_taken = end_time - start_time\n",
    "                print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "                \n",
    "                print(\"OUTPUT: \", response)\n",
    "\n",
    "                model_output1 = re.sub(r'\\[[^\\]]*\\]|\\.', '',response)\n",
    "\n",
    "                # Seprate sentences\n",
    "                sentences = model_output1.split(\". \")\n",
    "                # remove duplicates SENTENCES\n",
    "                unique_sentences = list( dict.fromkeys(sentences))\n",
    "\n",
    "                if not model_output1.endswith(\".\"):\n",
    "                # remove the last sentence if not . at last\n",
    "                    unique_sentences.pop()\n",
    "\n",
    "                # join unique sentences back into a text \n",
    "                model_output = \". \".join(unique_sentences)+ \".\"\n",
    "                \n",
    "                print(\"FINAL ANSWER: \", model_output1) \n",
    "\n",
    "                ans.append(model_output1)\n",
    "                qs.append(question)\n",
    "                ians.append(ideal_answer)\n",
    "\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                count+=1\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Question failed: {question}\")\n",
    "                retries += 1\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Question failed after {max_retries} attempts. Moving on to the next question.{e}\")\n",
    "                    qs.append(question)\n",
    "                    ians.append(ideal_answer)\n",
    "                    ans.append(\"\")\n",
    "                    break  # Break the retry loop and move to the next question\n",
    "                else:\n",
    "                    print(f\"Retrying question. Attempt {retries + 1} of {max_retries}.\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"question\"]=qs\n",
    "    df[\"answer\"]=ans\n",
    "    df[\"ideal_answer\"]=ians\n",
    "\n",
    "    df.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6532963-6176-4235-a553-003228e0565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: /home/sheetal/Project/lexicaleval.csv\n",
      "Question Column: question\n",
      "Idea Answer Column: ideal_answer\n",
      "Result File: result_Llama2\n",
      "---------- question How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How practically usable are existing tools for automating the refactoring to Microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 5972 tokens (5972 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "---------- question no: 0\n",
      "Question failed: What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "---------- question no: 0\n",
      "Question failed: What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "---------- question no: 0\n",
      "Question failed: What business-oriented quality goals should we prioritize during the migration? (e.g., improved time-to-market, better customer experience)\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4494 tokens (4494 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How can we ensure compatibility between existing systems and the new microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How can we ensure compatibility between existing systems and the new microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How can we ensure compatibility between existing systems and the new microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How can we ensure compatibility between existing systems and the new microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How can we ensure compatibility between existing systems and the new microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How can we ensure compatibility between existing systems and the new microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 2963 tokens (2963 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What measures can we take to maintain the quality of the system over time?\n",
      "---------- question no: 0\n",
      "Question failed: What measures can we take to maintain the quality of the system over time?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What measures can we take to maintain the quality of the system over time?\n",
      "---------- question no: 0\n",
      "Question failed: What measures can we take to maintain the quality of the system over time?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What measures can we take to maintain the quality of the system over time?\n",
      "---------- question no: 0\n",
      "Question failed: What measures can we take to maintain the quality of the system over time?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4986 tokens (4986 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What portability considerations are relevant for microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What portability considerations are relevant for microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What portability considerations are relevant for microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What portability considerations are relevant for microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What portability considerations are relevant for microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What portability considerations are relevant for microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 3618 tokens (3618 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How can we enhance reliability and scalability in the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: How can we enhance reliability and scalability in the new architecture?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How can we enhance reliability and scalability in the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: How can we enhance reliability and scalability in the new architecture?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How can we enhance reliability and scalability in the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: How can we enhance reliability and scalability in the new architecture?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 6723 tokens (6723 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What security measures should be in place for the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What security measures should be in place for the microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What security measures should be in place for the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What security measures should be in place for the microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What security measures should be in place for the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What security measures should be in place for the microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4997 tokens (4997 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How can we achieve autonomy for each microservice?\n",
      "---------- question no: 0\n",
      "Question failed: How can we achieve autonomy for each microservice?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How can we achieve autonomy for each microservice?\n",
      "---------- question no: 0\n",
      "Question failed: How can we achieve autonomy for each microservice?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How can we achieve autonomy for each microservice?\n",
      "---------- question no: 0\n",
      "Question failed: How can we achieve autonomy for each microservice?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 6196 tokens (6196 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What cohesion and coupling levels are desirable for the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: What cohesion and coupling levels are desirable for the new architecture?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What cohesion and coupling levels are desirable for the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: What cohesion and coupling levels are desirable for the new architecture?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What cohesion and coupling levels are desirable for the new architecture?\n",
      "---------- question no: 0\n",
      "Question failed: What cohesion and coupling levels are desirable for the new architecture?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4145 tokens (4145 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How do we manage complexity and granularity in the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How do we manage complexity and granularity in the microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How do we manage complexity and granularity in the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How do we manage complexity and granularity in the microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How do we manage complexity and granularity in the microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How do we manage complexity and granularity in the microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 5639 tokens (5639 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What isolation mechanisms can we implement to prevent interference between services?\n",
      "---------- question no: 0\n",
      "Question failed: What isolation mechanisms can we implement to prevent interference between services?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What isolation mechanisms can we implement to prevent interference between services?\n",
      "---------- question no: 0\n",
      "Question failed: What isolation mechanisms can we implement to prevent interference between services?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What isolation mechanisms can we implement to prevent interference between services?\n",
      "---------- question no: 0\n",
      "Question failed: What isolation mechanisms can we implement to prevent interference between services?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 3766 tokens (3766 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How should we handle technology heterogeneity across microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How should we handle technology heterogeneity across microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How should we handle technology heterogeneity across microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How should we handle technology heterogeneity across microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How should we handle technology heterogeneity across microservices?\n",
      "---------- question no: 0\n",
      "Question failed: How should we handle technology heterogeneity across microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 3882 tokens (3882 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "---------- question no: 0\n",
      "Question failed: What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "---------- question no: 0\n",
      "Question failed: What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "---------- question no: 0\n",
      "Question failed: What guidelines or workflows can we establish to guide the migration process? How can we document these effectively?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 11924 tokens (11924 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "---------- question no: 0\n",
      "Question failed: How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "---------- question no: 0\n",
      "Question failed: How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "---------- question no: 0\n",
      "Question failed: How should we represent the list of services that will replace the monolith? Is there a preferred format or structure?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 13280 tokens (13280 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "---------- question no: 0\n",
      "Question failed: Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "---------- question no: 0\n",
      "Question failed: Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "---------- question no: 0\n",
      "Question failed: Can we extract relevant portions of the monolith’s source code to aid in microservice creation?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4495 tokens (4495 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "---------- question no: 0\n",
      "Question failed: What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "---------- question no: 0\n",
      "Question failed: What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "---------- question no: 0\n",
      "Question failed: What recommendations exist for splitting the monolith into smaller services? Are there best practices or patterns?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4648 tokens (4648 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How can we visualize the microservices and their interactions?\n",
      "---------- question no: 0\n",
      "Question failed: How can we visualize the microservices and their interactions?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How can we visualize the microservices and their interactions?\n",
      "---------- question no: 0\n",
      "Question failed: How can we visualize the microservices and their interactions?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How can we visualize the microservices and their interactions?\n",
      "---------- question no: 0\n",
      "Question failed: How can we visualize the microservices and their interactions?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 11135 tokens (11135 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: The monolith exposes APIs or interfaces. How can we break them down into microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 11858 tokens (11858 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "---------- question no: 0\n",
      "Question failed: How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "---------- question no: 0\n",
      "Question failed: How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "---------- question no: 0\n",
      "Question failed: How to deal with database schema or data storage mechanisms used by the monolith?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 11141 tokens (11141 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "---------- question no: 0\n",
      "Question failed: How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "---------- question no: 0\n",
      "Question failed: How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "---------- question no: 0\n",
      "Question failed: How is the monolith’s source code refactored into smaller, cohesive services?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 5632 tokens (5632 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What test cases exist for the monolith, and how can we validate microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What test cases exist for the monolith, and how can we validate microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What test cases exist for the monolith, and how can we validate microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What test cases exist for the monolith, and how can we validate microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What test cases exist for the monolith, and how can we validate microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What test cases exist for the monolith, and how can we validate microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 13274 tokens (13274 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What is the best tool in java to refactor our application into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is the best tool in java to refactor our application into microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What is the best tool in java to refactor our application into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is the best tool in java to refactor our application into microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What is the best tool in java to refactor our application into microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is the best tool in java to refactor our application into microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4375 tokens (4375 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What is Docker?\n",
      "---------- question no: 0\n",
      "Question failed: What is Docker?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What is Docker?\n",
      "---------- question no: 0\n",
      "Question failed: What is Docker?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What is Docker?\n",
      "---------- question no: 0\n",
      "Question failed: What is Docker?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 2553 tokens (2553 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What is an API Gateway?\n",
      "---------- question no: 0\n",
      "Question failed: What is an API Gateway?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What is an API Gateway?\n",
      "---------- question no: 0\n",
      "Question failed: What is an API Gateway?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What is an API Gateway?\n",
      "---------- question no: 0\n",
      "Question failed: What is an API Gateway?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4694 tokens (4694 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What is Service Discovery in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Service Discovery in microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What is Service Discovery in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Service Discovery in microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What is Service Discovery in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Service Discovery in microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 2184 tokens (2184 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What is Circuit Breaking in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Circuit Breaking in microservices?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What is Circuit Breaking in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Circuit Breaking in microservices?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What is Circuit Breaking in microservices?\n",
      "---------- question no: 0\n",
      "Question failed: What is Circuit Breaking in microservices?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 4524 tokens (4524 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------- question What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "---------- question no: 0\n",
      "Question failed: What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "Retrying question. Attempt 2 of 3.\n",
      "---------- question What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "---------- question no: 0\n",
      "Question failed: What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "Retrying question. Attempt 3 of 3.\n",
      "---------- question What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "---------- question no: 0\n",
      "Question failed: What do you recommend? Migration of services should be in smaller steps or all at once?\n",
      "Question failed after 3 attempts. Moving on to the next question.Error code: 400 - {'error': {'message': \"This model's maximum context length is 2048 tokens. However, you requested 11142 tokens (11142 in the messages, None in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "human_evaluation_script('/home/sheetal/Project/lexicaleval.csv', \"question\", \"ideal_answer\", \"result_Llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293d48-74ad-42ed-acc9-b43a7965836b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
