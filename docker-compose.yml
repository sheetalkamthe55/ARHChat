services:
  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
  mongo-express:
    image: mongodb/mongodb-community-server:latest
    restart: always
    ports:
      - 27017:27017
  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    volumes:
      - ./qdrant_data:/qdrant_data
  streamlit:
    build:
      dockerfile: Dockerfile-gpu
    depends_on:
      - mongo-express
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: 'all'
    working_dir: /app/ARHChat
    ports:
      - 8503:8503
    volumes:
      - ~/Project/ARHChat/metadata:/app/ARHChat/metadata
      - ~/Project/ARHChat/.streamlit/secrets.toml:/app/ARHChat/.streamlit/secrets.toml
    command: ["streamlit", "run", "app.py", "--server.port=8503", "--server.address=0.0.0.0"]
  
  llama_cpp_server:
    image: ghcr.io/allenporter/llama-cpp-server-cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: 'all'
    environment:
      CUDA_VISIBLE_DEVICES: 1
    command: ["__main__.py", "--model", "/tmp/llama_index/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf", "--port", "8009", "--host", "0.0.0.0", "--n_ctx", "8000", "--chat_format", "chatml"]
    ports:
      - "8009:8009"
    volumes:
      - ./tmp/llama_index/models:/tmp/llama_index/models