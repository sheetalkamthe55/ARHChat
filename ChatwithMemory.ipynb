{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467ae64e-262d-4bc0-9e18-64e3014c925d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==23.2.1\n",
      "aiohttp==3.9.3\n",
      "aiosignal==1.3.1\n",
      "altair==5.3.0\n",
      "annotated-types==0.6.0\n",
      "anyio==3.7.1\n",
      "appnope==0.1.3\n",
      "asttokens==2.2.1\n",
      "async-timeout==4.0.3\n",
      "asyncer==0.0.2\n",
      "attrs==23.2.0\n",
      "backcall==0.2.0\n",
      "bidict==0.23.1\n",
      "blinker==1.7.0\n",
      "build==1.0.3\n",
      "CacheControl==0.13.1\n",
      "cachetools==5.3.3\n",
      "certifi==2023.7.22\n",
      "cffi==1.16.0\n",
      "chainlit==1.0.301\n",
      "charset-normalizer==3.3.2\n",
      "cleo==2.1.0\n",
      "click==8.1.7\n",
      "comm==0.1.3\n",
      "contourpy==1.0.7\n",
      "crashtest==0.4.1\n",
      "cycler==0.11.0\n",
      "dataclasses-json==0.5.14\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "Deprecated==1.2.14\n",
      "distlib==0.3.8\n",
      "distro==1.9.0\n",
      "dnspython==2.4.2\n",
      "dulwich==0.21.7\n",
      "exceptiongroup==1.2.0\n",
      "executing==1.2.0\n",
      "fastapi==0.108.0\n",
      "fastapi-socketio==0.0.10\n",
      "fastjsonschema==2.19.1\n",
      "filelock==3.13.1\n",
      "filetype==1.2.0\n",
      "fonttools==4.39.3\n",
      "frozenlist==1.4.1\n",
      "fsspec==2023.10.0\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.43\n",
      "googleapis-common-protos==1.62.0\n",
      "grpcio==1.64.0\n",
      "grpcio-tools==1.62.2\n",
      "h11==0.14.0\n",
      "h2==4.1.0\n",
      "hpack==4.0.0\n",
      "httpcore==1.0.4\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.17.3\n",
      "hyperframe==6.0.1\n",
      "idna==3.4\n",
      "importlib-metadata==6.11.0\n",
      "installer==0.7.0\n",
      "ipykernel==6.22.0\n",
      "ipython==8.13.1\n",
      "jaraco.classes==3.3.1\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter_client==8.2.0\n",
      "jupyter_core==5.3.0\n",
      "kazoo==2.5.0\n",
      "keyring==24.3.0\n",
      "kiwisolver==1.4.4\n",
      "langchain==0.1.16\n",
      "langchain-community==0.0.34\n",
      "langchain-core==0.1.50\n",
      "langchain-openai==0.1.1\n",
      "langchain-postgres==0.0.7\n",
      "langchain-text-splitters==0.0.2\n",
      "langsmith==0.1.52\n",
      "Lazify==0.4.0\n",
      "literalai==0.0.204\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.3\n",
      "marshmallow==3.21.0\n",
      "matplotlib==3.7.1\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "more-itertools==10.2.0\n",
      "mpmath==1.3.0\n",
      "msgpack==1.0.7\n",
      "multidict==6.0.5\n",
      "mypy-extensions==1.0.0\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "numpy==1.24.3\n",
      "openai==1.30.3\n",
      "opentelemetry-api==1.23.0\n",
      "opentelemetry-exporter-otlp==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-http==1.23.0\n",
      "opentelemetry-instrumentation==0.44b0\n",
      "opentelemetry-proto==1.23.0\n",
      "opentelemetry-sdk==1.23.0\n",
      "opentelemetry-semantic-conventions==0.44b0\n",
      "orjson==3.10.3\n",
      "packaging==23.2\n",
      "pandas==2.0.1\n",
      "parso==0.8.3\n",
      "pexpect==4.8.0\n",
      "pgvector==0.2.5\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.5.0\n",
      "pkginfo==1.9.6\n",
      "platformdirs==3.11.0\n",
      "poetry==1.7.1\n",
      "poetry-core==1.8.1\n",
      "poetry-plugin-export==1.6.0\n",
      "portalocker==2.8.2\n",
      "prompt-toolkit==3.0.38\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.5\n",
      "psycopg==3.1.19\n",
      "psycopg-binary==3.1.19\n",
      "psycopg-pool==3.2.2\n",
      "psycopg2-binary==2.9.9\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyarrow==16.0.0\n",
      "pycparser==2.21\n",
      "pydantic==2.6.3\n",
      "pydantic_core==2.16.3\n",
      "pydeck==0.8.1b0\n",
      "Pygments==2.15.1\n",
      "PyJWT==2.8.0\n",
      "pykafka==2.8.0\n",
      "pymongo==4.7.2\n",
      "pyparsing==3.0.9\n",
      "pyproject_hooks==1.0.0\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.1\n",
      "python-engineio==4.9.0\n",
      "python-graphql-client==0.4.3\n",
      "python-multipart==0.0.6\n",
      "python-socketio==5.11.1\n",
      "pytz==2023.3\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.0.2\n",
      "qdrant-client==1.8.2\n",
      "rapidfuzz==3.6.1\n",
      "referencing==0.34.0\n",
      "regex==2023.10.3\n",
      "requests==2.31.0\n",
      "requests-toolbelt==1.0.0\n",
      "rich==13.7.1\n",
      "rpds-py==0.18.0\n",
      "safetensors==0.4.0\n",
      "scikit-learn==1.2.2\n",
      "scipy==1.10.1\n",
      "seaborn==0.12.2\n",
      "sentence-transformers==2.6.1\n",
      "shellingham==1.5.4\n",
      "simple-websocket==1.0.0\n",
      "six==1.16.0\n",
      "sklearn==0.0.post4\n",
      "smmap==5.0.1\n",
      "sniffio==1.3.1\n",
      "SQLAlchemy==2.0.30\n",
      "stack-data==0.6.2\n",
      "starlette==0.32.0.post1\n",
      "streamlit==1.33.0\n",
      "streamlit-server-state==0.17.1\n",
      "sympy==1.12\n",
      "syncer==2.0.3\n",
      "tabulate==0.9.0\n",
      "tenacity==8.2.3\n",
      "threadpoolctl==3.1.0\n",
      "tiktoken==0.7.0\n",
      "tokenizers==0.14.1\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "tomlkit==0.12.3\n",
      "toolz==0.12.1\n",
      "torch==2.1.0\n",
      "torchaudio==2.1.0\n",
      "torchvision==0.16.0\n",
      "tornado==6.3.1\n",
      "tqdm==4.66.1\n",
      "traitlets==5.9.0\n",
      "transformers==4.35.0\n",
      "trove-classifiers==2024.1.31\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.8.0\n",
      "tzdata==2023.3\n",
      "uptrace==1.22.0\n",
      "urllib3==2.1.0\n",
      "uvicorn==0.25.0\n",
      "virtualenv==20.25.0\n",
      "watchfiles==0.20.0\n",
      "wcwidth==0.2.6\n",
      "websockets==12.0\n",
      "wrapt==1.16.0\n",
      "wsproto==1.2.0\n",
      "xattr==0.10.1\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7da656-04c9-4827-85ab-bca5e134f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymongo[srv]\n",
      "  Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.0/670.0 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.6.1 pymongo-4.7.2\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06262ece-7334-4682-b587-01c02144c808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyopenssl in /usr/lib/python3/dist-packages (21.0.0)\n",
      "Collecting pyopenssl\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography<43,>=41.0.5 in ./.local/lib/python3.10/site-packages (from pyopenssl) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/lib/python3/dist-packages (from cryptography<43,>=41.0.5->pyopenssl) (1.15.0)\n",
      "Installing collected packages: pyopenssl\n",
      "Successfully installed pyopenssl-24.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyopenssl --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3b49d2-5127-4d80-9d94-47af0930948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MongoDBChatMessageHistory  import MongoDBChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b55d35-8dfe-4d82-b37d-b96df4687b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib \n",
    "MONGODB_URI = \"mongodb://localhost:27017\"\n",
    "DB_NAME = \"ARH_chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94559ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mclient = MongoClient(MONGODB_URI)\n",
    "database = mclient[\"ARH_chatbot\"]\n",
    "database.list_collection_names()\n",
    "collection = database[\"chat_history\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33d91c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('66787b9dcbd3da2cb01472b5'), 'SessionId': 'c8d4b195-971c-48a5-8f1c-b249acdc53ba', 'UserId': 'admin', 'History': '{\"type\": \"human\", \"data\": {\"content\": \"What is BPM. Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}'}\n",
      "{'_id': ObjectId('66787b9dcbd3da2cb01472b6'), 'SessionId': 'c8d4b195-971c-48a5-8f1c-b249acdc53ba', 'UserId': 'admin', 'History': '{\"type\": \"ai\", \"data\": {\"content\": \"Business Process Management (BPM) refers to the systematic management and optimization of business processes, enabling organizations to improve efficiency, reduce costs, and enhance customer satisfaction through process automation, monitoring, and continuous improvement.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}'}\n",
      "{'_id': ObjectId('66787f86cbd3da2cb01472b8'), 'SessionId': 'c8d4b195-971c-48a5-8f1c-b249acdc53ba', 'UserId': 'admin', 'History': '{\"type\": \"human\", \"data\": {\"content\": \"What is an API. Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}'}\n",
      "{'_id': ObjectId('66787f86cbd3da2cb01472b9'), 'SessionId': 'c8d4b195-971c-48a5-8f1c-b249acdc53ba', 'UserId': 'admin', 'History': '{\"type\": \"ai\", \"data\": {\"content\": \"An Application Programming Interface (API) is a set of defined rules that enables different applications, systems, or services to communicate with each other and exchange data in a structured and standardized way.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}'}\n",
      "{'_id': ObjectId('66787fa5cbd3da2cb01472bb'), 'SessionId': '8889770d-d5d0-406e-9c63-e6716fda47e0', 'UserId': 'sheetal', 'History': '{\"type\": \"human\", \"data\": {\"content\": \"What is Microservice. Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}'}\n",
      "{'_id': ObjectId('66787fa5cbd3da2cb01472bc'), 'SessionId': '8889770d-d5d0-406e-9c63-e6716fda47e0', 'UserId': 'sheetal', 'History': '{\"type\": \"ai\", \"data\": {\"content\": \"A microservice is a small, independent service that performs a specific business capability or function within an overall system, designed to be scalable, maintainable, and loosely coupled with other services.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}'}\n"
     ]
    }
   ],
   "source": [
    "collection.count_documents({})\n",
    "cursor = collection.find({})\n",
    "for document in cursor:\n",
    "          print(document)\n",
    "mclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02e7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'History': ['{\"type\": \"human\", \"data\": {\"content\": \"What is a Docker container? Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}', '{\"type\": \"ai\", \"data\": {\"content\": \"A Docker container is a lightweight and portable software package that includes an application, its dependencies, and the necessary runtime environment to run it, all packaged together within a single file.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}'], '_id': '0e796a7c-42ff-4277-b982-383b9987eb20', 'user_id': 'admin'}, {'History': ['{\"type\": \"human\", \"data\": {\"content\": \"What is BPM. Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}', '{\"type\": \"ai\", \"data\": {\"content\": \"Business Process Management (BPM) refers to the systematic management and optimization of business processes, enabling organizations to improve efficiency, reduce costs, and enhance customer satisfaction through process automation, monitoring, and continuous improvement.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}', '{\"type\": \"human\", \"data\": {\"content\": \"What is an API. Explain in 1 sentence\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}', '{\"type\": \"ai\", \"data\": {\"content\": \"An Application Programming Interface (API) is a set of defined rules that enables different applications, systems, or services to communicate with each other and exchange data in a structured and standardized way.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}}'], '_id': 'c8d4b195-971c-48a5-8f1c-b249acdc53ba', 'user_id': 'admin'}]\n",
      "0e796a7c-42ff-4277-b982-383b9987eb20\n",
      "What is a Docker container? Explain in 1 sentence\n",
      "A Docker container is a lightweight and portable software package that includes an application, its dependencies, and the necessary runtime environment to run it, all packaged together within a single file.\n",
      "c8d4b195-971c-48a5-8f1c-b249acdc53ba\n",
      "What is BPM. Explain in 1 sentence\n",
      "Business Process Management (BPM) refers to the systematic management and optimization of business processes, enabling organizations to improve efficiency, reduce costs, and enhance customer satisfaction through process automation, monitoring, and continuous improvement.\n",
      "What is an API. Explain in 1 sentence\n",
      "An Application Programming Interface (API) is a set of defined rules that enables different applications, systems, or services to communicate with each other and exchange data in a structured and standardized way.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "user_id = 'admin'\n",
    "\n",
    "# MongoDB aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        '$match': {\n",
    "            'UserId': user_id  # Filter documents by UserId\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$SessionId',  # Group by SessionId\n",
    "            'user_id': {'$first': '$UserId'},  # Example: Get the first UserId in each group\n",
    "            'count': {'$sum': 1},  # Example: Count documents in each group\n",
    "            'History': {'$push': '$History'}  # Example: Push History field into an array\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$sort': {'_id': 1}  # Optional: Sort by SessionId (ascending)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation pipeline\n",
    "results = collection.aggregate(pipeline)\n",
    "# for result in results:\n",
    "#     print(result)\n",
    "\n",
    "if results:\n",
    "    items = [{\"History\": document[\"History\"], \"_id\": document[\"_id\"], \"user_id\": document[\"user_id\"]} for document in results]\n",
    "else:\n",
    "    items = []\n",
    "\n",
    "print(items)\n",
    "\n",
    "for chat_index, chat in enumerate(items, start=1):\n",
    "    chat_label = f\"Chat {chat_index}\"\n",
    "    parsed_messages = [json.loads(message) for message in chat[\"History\"]]\n",
    "        # Display each message in an individual expander\n",
    "    print(chat[\"_id\"])\n",
    "    for message_index, message in enumerate(parsed_messages, start=1):\n",
    "            message_label = f\"{message['type'].capitalize()} {message_index}: {message['data']['content'][:50]}...\"\n",
    "            print(message['data']['content'])\n",
    "\n",
    "# parsed_items = [json.loads(item) for item in items[0]]\n",
    "\n",
    "# Use Streamlit's expander to display each item\n",
    "# for i, item in enumerate(parsed_items, start=1):\n",
    "#    with st.expander(f\"{item['type'].capitalize()} {i}: {item['data']['content'][:50]}...\"):\n",
    "#         st.write(item['data']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e50de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 4, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27da96d-60ef-4a18-993a-c1c05348c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbutils.MongoDBChatMessageHistory import MongoDBChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str,user_id: str) -> MongoDBChatMessageHistory:\n",
    "        return MongoDBChatMessageHistory(MONGODB_URI, session_id,user_id, database_name=DB_NAME, collection_name=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97a910b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'human',\n",
       "  'data': {'content': 'What is BPM. Explain in 1 sentence',\n",
       "   'additional_kwargs': {},\n",
       "   'response_metadata': {},\n",
       "   'type': 'human',\n",
       "   'name': None,\n",
       "   'id': None,\n",
       "   'example': False}},\n",
       " {'type': 'ai',\n",
       "  'data': {'content': 'Business Process Management (BPM) refers to the systematic management and optimization of business processes, enabling organizations to improve efficiency, reduce costs, and enhance customer satisfaction through process automation, monitoring, and continuous improvement.',\n",
       "   'additional_kwargs': {},\n",
       "   'response_metadata': {},\n",
       "   'type': 'ai',\n",
       "   'name': None,\n",
       "   'id': None,\n",
       "   'example': False,\n",
       "   'tool_calls': [],\n",
       "   'invalid_tool_calls': []}},\n",
       " {'type': 'human',\n",
       "  'data': {'content': 'What is an API. Explain in 1 sentence',\n",
       "   'additional_kwargs': {},\n",
       "   'response_metadata': {},\n",
       "   'type': 'human',\n",
       "   'name': None,\n",
       "   'id': None,\n",
       "   'example': False}},\n",
       " {'type': 'ai',\n",
       "  'data': {'content': 'An Application Programming Interface (API) is a set of defined rules that enables different applications, systems, or services to communicate with each other and exchange data in a structured and standardized way.',\n",
       "   'additional_kwargs': {},\n",
       "   'response_metadata': {},\n",
       "   'type': 'ai',\n",
       "   'name': None,\n",
       "   'id': None,\n",
       "   'example': False,\n",
       "   'tool_calls': [],\n",
       "   'invalid_tool_calls': []}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"\", \"\").getformatedmessage(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c063c9-5b9f-4174-95b5-5c0d4018f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "client = QdrantClient(host=\"129.69.217.24\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98b924b-0518-45d5-9c2c-64ebb26ead00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embed_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74da6f37-e38c-45e4-b42f-8fe570c30c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant(client=client, collection_name=\"ARH_Tool\", embeddings=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b248093a-a02a-42ed-9578-97f2d704b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18734f8d-8f34-42b4-82fb-a59cbda0f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server_url = \"http://129.69.217.24:8009/v1\"\n",
    "\n",
    "# create LLaamacpp Langchain instance\n",
    "model = ChatOpenAI(\n",
    "    model=\"no-path\",\n",
    "    openai_api_key=\"no-key\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    # max_tokens=5,\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01505e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_docs_with_id(docs) -> str:\n",
    "#     formatted = [\n",
    "#         f\"Source ID: {i}\\nArticle Title: {doc.metadata['source']}\\nArticle Snippet: {doc.page_content}\"\n",
    "#         for i, doc in enumerate(docs)\n",
    "#     ]\n",
    "#     return \"\\n\\n\" + \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22098ea-19c7-43ca-9e81-ff08124f0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a follow-up question and history, create a standalone question\n",
    "standalone_system_prompt = \"\"\"\n",
    "Given a chat history and a follow-up question, rephrase the follow-up question to be a standalone question. \\\n",
    "Do NOT answer the question, just reformulate it if needed, otherwise return it as is. \\\n",
    "Only return the final standalone question. \\\n",
    "\"\"\"\n",
    "standalone_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", standalone_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "parse_output = StrOutputParser()\n",
    "\n",
    "question_chain =  standalone_question_prompt | model  | parse_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82f2345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_chain = (lambda x: x[\"question\"]) | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35aebb38-4f0a-4ab5-b7fa-d8cb34bea9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context by passing output of the question_chain i.e. the standalone question to the retriever\n",
    "retriever_chain = RunnablePassthrough.assign(context=question_chain | retriever | (lambda docs: {\n",
    "        'content': \"\\n\\n\".join([d.page_content for d in docs]),\n",
    "        'sources': [d.metadata['source'] for d in docs]\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19b5d14-a5eb-4384-a3fd-590bde69031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt that includes the context, history and the follow-up question\n",
    "rag_system_prompt = \"\"\"Answer the question based only on the following context: \\\n",
    "{context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a624ec-ffc2-46bc-8172-6a91903d9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global variable to hold the output of retriever_chain\n",
    "retriever_output = None\n",
    "\n",
    "# Define a function to capture the output of retriever_chain\n",
    "def capture_output(output):\n",
    "    global retriever_output\n",
    "    retriever_output = output\n",
    "    return output\n",
    "\n",
    "# RAG chain\n",
    "# rag_chain = (\n",
    "#     retriever_chain\n",
    "#     | RunnablePassthrough.assign(prompt=rag_prompt)\n",
    "#     | RunnablePassthrough.assign(response=lambda inputs: model(inputs[\"prompt\"].messages))\n",
    "#     | RunnablePassthrough.assign(what=parse_output)\n",
    "# )\n",
    "\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | capture_output \n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | parse_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b571b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_chain = RunnablePassthrough.assign(context=retriever_chain).assign(\n",
    "#     answer=rag_chain\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd48a74-3d57-415b-b0b5-8a629a3efc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RAG chain with history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    output_messages_key=\"answer\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e32b23f-69fa-4be0-bb66-7fa3c7adf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 6 µs, total: 15 µs\n",
      "Wall time: 25 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object RunnableBindingBase.stream at 0x71f0ea654c10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with_message_history.stream({\"question\": \"How is the microservice deployed into the production environment?\"}, {\"configurable\": {\"session_id\": \"1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55f33943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eededd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'chat_history', 'type': 'collection', 'options': {}, 'info': {'readOnly': False, 'uuid': Binary(b'\\x8d\\x1e\\x7f:!\\xc9K\\x95\\xad\\xc2\\xfbNf\\x8d%j', 4)}, 'idIndex': {'v': 2, 'key': {'_id': 1}, 'name': '_id_'}}\n"
     ]
    }
   ],
   "source": [
    "database = client.get_database('ARH_chatbot') \n",
    "collection_list = database.list_collections()\n",
    "for c in collection_list:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9df11bb1-5945-4beb-8319-eaf3e0777f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e22cddf8-89f7-4ebb-b25e-5e680bbbe757 not found for run abc52ebc-8016-489f-a6c1-4142173442c8. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BPM stands for Business Process Management. It refers to the management of business processes, which are sets of activities that create value for an organization by transforming inputs into outputs.\\n\\nBusiness process management involves designing, modeling, executing, and monitoring business processes to achieve specific goals and objectives. BPM aims to improve the efficiency, effectiveness, and agility of business processes by:\\n\\n1. Identifying and documenting existing processes\\n2. Analyzing and optimizing processes to eliminate waste and inefficiencies\\n3. Automating repetitive tasks and manual processes\\n4. Implementing controls and monitoring systems to ensure compliance and quality\\n5. Continuously improving and refining processes based on feedback and performance metrics\\n\\nBPM is used in various industries, including finance, healthcare, manufacturing, logistics, and government, among others. It helps organizations to:\\n\\n* Increase productivity and efficiency\\n* Reduce costs and improve profitability\\n* Enhance customer satisfaction and loyalty\\n* Improve compliance with regulations and standards\\n* Gain a competitive advantage through process innovation\\n\\nSome common BPM tools and technologies include workflow management systems, business process modeling languages (BPMN), and business rules engines. These tools help organizations to design, implement, and manage their business processes more effectively.\\n\\nIn the context of the provided text, BPM is not explicitly mentioned, but it can be related to the concept of microservices architecture, which involves breaking down monolithic applications into smaller, independent services that communicate with each other through APIs. This approach requires careful process management to ensure seamless integration and efficient communication between microservices.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = with_message_history.invoke(\n",
    "    {\"question\": \"What is BPM?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\",\"user\": \"user1\"}},\n",
    ")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a44898ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'sources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ans.keys()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mretriever_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'sources'"
     ]
    }
   ],
   "source": [
    "# ans.keys()\n",
    "\n",
    "retriever_output[\"context\"][\"sources\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e80b2d-23c3-414e-bf1e-dc61201ef261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "chat_history=[]\n",
    "for jsonpatch_op in with_message_history.stream(\n",
    "    {\"question\": \"How is the microservice deployed into the production environment?\"}, {\"configurable\": {\"session_id\": \"1\"}}\n",
    "):\n",
    "    print(jsonpatch_op)\n",
    "    # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    # ct += 1\n",
    "    # if ct > 20:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "720094b8-f893-43ac-a164-96ce52b4647a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='When was he elected?'),\n",
       " AIMessage(content=\"I apologize, but there is no information about an election in the provided context. The text appears to be a passage from a research paper or article about software engineering and microservices architecture. It does not mention any individual being elected. If you could provide more context or clarify what you are referring to, I'll do my best to help.\"),\n",
       " HumanMessage(content='When was he elected?'),\n",
       " AIMessage(content=\"I apologize again, but there is no information about an election or a person being elected in the provided context. The text appears to be a passage from a research paper or article about software engineering and microservices architecture, and does not mention any individual being elected. If you could provide more context or clarify what you are referring to, I'll do my best to help.\")]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"abc123\").messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35995493-30e4-4cff-b165-b8dcf77fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"callback-experiments\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "efd3d86c-475e-4683-b1f9-3f14e698ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1320d0f2-78cc-4c80-b5bf-590fb122f873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "import os\n",
    "cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a32f67e-ec7f-42fa-8933-fad71b3bc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2752d7a5-a12f-42cb-ac35-45f559e3c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb8142b4-2076-481e-9a83-de850e8dc122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /tmp/llama_index/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      "warning: failed to mlock 369520640-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1000.00 MiB, K (f16):  500.00 MiB, V (f16):  500.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   669.48 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    23.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 356\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1467355\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://129.69.217.24:8009\u001b[0m (Press CTRL+C to quit)\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1467355\u001b[0m]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/llama_cpp/server/__main__.py\", line 97, in <module>\n",
      "    main()\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/llama_cpp/server/__main__.py\", line 87, in main\n",
      "    uvicorn.run(\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/uvicorn/main.py\", line 575, in run\n",
      "    server.run()\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"uvloop/loop.pyx\", line 1511, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1504, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1377, in uvloop.loop.Loop.run_forever\n",
      "  File \"uvloop/loop.pyx\", line 555, in uvloop.loop.Loop._run\n",
      "  File \"uvloop/loop.pyx\", line 474, in uvloop.loop.Loop._on_idle\n",
      "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
      "  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/uvicorn/server.py\", line 68, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/sheetal/.local/lib/python3.10/site-packages/uvicorn/server.py\", line 328, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 -m llama_cpp.server --model /tmp/llama_index/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf --port 8009 --host 129.69.217.24 --n_ctx 8000 --chat_format chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cb807ee-ead0-4949-b6dd-3183d7838731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b205a9ad-a6ba-4bd7-bb04-e5f685d3a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 18:17:02.508 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/lib/python3/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page title\n",
    "st.set_page_config(page_title='Architecture Refactoring AI Helper')\n",
    "st.title('Architecture Refactoring AI Helper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4266a05-4842-4f6d-819d-253b8dad8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  9 14:12:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   36C    P8              20W / 450W |  22861MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:61:00.0 Off |                  Off |\n",
      "|  0%   35C    P8              12W / 450W |    416MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1532      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1789      G   /usr/bin/gnome-shell                         12MiB |\n",
      "|    0   N/A  N/A   2343488      C   /usr/bin/python3                            930MiB |\n",
      "|    0   N/A  N/A   2745747      C   /home/kasra/metal-engine/bin/python       21840MiB |\n",
      "|    1   N/A  N/A      1532      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A   2745747      C   /home/kasra/metal-engine/bin/python         384MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236c207f-a113-4e4c-bbe5-7992ee9c7e79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2872089/2540486483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ba6a4-bf17-4e65-a5c6-180df1bc77cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
